{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_train = \"../../../data/bosch/\"\n",
    "with open(os.path.join(path_train ,\"train.yaml\"), 'r') as stream:\n",
    "    try:\n",
    "        data = yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "        \n",
    "labels = [\"Red\", \"Yellow\", \"Green\"]\n",
    "train_data = []\n",
    "for d in data:\n",
    "    to_append = {\"path\" : d[\"path\"], \"boxes\" : []}\n",
    "    for b in d[\"boxes\"]:\n",
    "        if (b[\"occluded\"] is False) and (b[\"label\"] in labels):\n",
    "            to_append[\"boxes\"].append(b)\n",
    "    if len(to_append[\"boxes\"]) > 0:\n",
    "        train_data.append(to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_train_add = \"../../../data/bosch_additional/\"\n",
    "with open(os.path.join(path_train_add ,\"additional_train.yaml\"), 'r') as stream:\n",
    "    try:\n",
    "        data = yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "train_data_add = []\n",
    "labels = [\"Red\", \"Yellow\", \"Green\"]\n",
    "for d in data:\n",
    "    to_append = {\"path\" : d[\"path\"], \"boxes\" : []}\n",
    "    for b in d[\"boxes\"]:\n",
    "        if (b[\"occluded\"] is False) and (b[\"label\"] in labels):\n",
    "            to_append[\"boxes\"].append(b)\n",
    "    if len(to_append[\"boxes\"]) > 0:\n",
    "        train_data_add.append(to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_test = \"../../../data/bosch_test/\"\n",
    "with open(os.path.join(path_test ,\"test.yaml\"), 'r') as stream:\n",
    "    try:\n",
    "        data = yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "test_data = []\n",
    "labels = [\"Red\", \"Yellow\", \"Green\"]\n",
    "for d in data:\n",
    "    to_append = {\"path\" : d[\"path\"], \"boxes\" : []}\n",
    "    for b in d[\"boxes\"]:\n",
    "        if (b[\"occluded\"] is False) and (b[\"label\"] in labels):\n",
    "            to_append[\"boxes\"].append(b)\n",
    "    if len(to_append[\"boxes\"]) > 0:\n",
    "        test_data.append(to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we extract the bounding boxes \n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "path_save = \"../../../data/traffic_lights_from_bosch/\"\n",
    "count = 0\n",
    "for d in train_data:\n",
    "    image = cv2.imread(os.path.join(path_train, d[\"path\"]))\n",
    "    for tl in d[\"boxes\"]:\n",
    "        x_min = max(int(tl[\"x_min\"]), 0) \n",
    "        x_max = max(int(tl[\"x_max\"]), 0)\n",
    "        y_min = max(int(tl[\"y_min\"]), 0)\n",
    "        y_max = max(int(tl[\"y_max\"]), 0)\n",
    "        deltax = x_max - x_min\n",
    "        if (deltax > 20) and (deltax < 40):                   #If the traffic light in the image is already big enough\n",
    "            miss = 40 - deltax\n",
    "            for i in range(5):\n",
    "                wide_x_up = random.randint(0, miss)\n",
    "                wide_x_down = miss - wide_x_up\n",
    "                y_center = (y_max + y_min) //2\n",
    "                cropped = image[y_center - 25 : y_center + 25 , x_min - wide_x_down - 5 : x_max + wide_x_up + 5, :]\n",
    "                if cropped.shape == (50, 50, 3):\n",
    "                    filename = str(count).zfill(5) + \"_\" + str(tl[\"label\"]) + \".png\"\n",
    "                    count += 1\n",
    "                    cv2.imwrite(os.path.join(path_save, str(tl[\"label\"]) , filename), cropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for d in train_data_add:\n",
    "    image = cv2.imread(os.path.join(path_train_add, d[\"path\"]))\n",
    "    for tl in d[\"boxes\"]:\n",
    "        x_min = max(int(tl[\"x_min\"]), 0) \n",
    "        x_max = max(int(tl[\"x_max\"]), 0)\n",
    "        y_min = max(int(tl[\"y_min\"]), 0)\n",
    "        y_max = max(int(tl[\"y_max\"]), 0)\n",
    "        deltax = x_max - x_min\n",
    "        if (deltax > 20) and (deltax < 40):                   #If the traffic light in the image is already big enough\n",
    "            miss = 40 - deltax\n",
    "            for i in range(5):\n",
    "                wide_x_up = random.randint(0, miss)\n",
    "                wide_x_down = miss - wide_x_up\n",
    "                y_center = (y_max + y_min) //2\n",
    "                cropped = image[y_center - 25 : y_center + 25 , x_min - wide_x_down - 5 : x_max + wide_x_up + 5, :]\n",
    "                if cropped.shape == (50, 50, 3):\n",
    "                    filename = str(count).zfill(5) + \"_\" + str(tl[\"label\"]) + \".png\"\n",
    "                    count += 1\n",
    "                    cv2.imwrite(os.path.join(path_save, str(tl[\"label\"]) , filename), cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for d in test_data:\n",
    "    image = cv2.imread(os.path.join(path_test, \"rgb/test\", os.path.split(d[\"path\"])[1]))\n",
    "    for tl in d[\"boxes\"]:\n",
    "        x_min = max(int(tl[\"x_min\"]), 0) \n",
    "        x_max = max(int(tl[\"x_max\"]), 0)\n",
    "        y_min = max(int(tl[\"y_min\"]), 0)\n",
    "        y_max = max(int(tl[\"y_max\"]), 0)\n",
    "        deltax = x_max - x_min\n",
    "        if (deltax > 20) and (deltax < 40):                   #If the traffic light in the image is already big enough\n",
    "            miss = 40 - deltax\n",
    "            for i in range(5):\n",
    "                wide_x_up = random.randint(0, miss)\n",
    "                wide_x_down = miss - wide_x_up\n",
    "                y_center = (y_max + y_min) //2\n",
    "                cropped = image[y_center - 25 : y_center + 25 , x_min - wide_x_down - 5 : x_max + wide_x_up + 5, :]\n",
    "                if cropped.shape == (50, 50, 3):\n",
    "                    filename = str(count).zfill(5) + \"_\" + str(tl[\"label\"]) + \".png\"\n",
    "                    count += 1\n",
    "                    cv2.imwrite(os.path.join(path_save, str(tl[\"label\"]) , filename), cropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_save = \"../../../data/traffic_lights_from_bosch/\"\n",
    "y = []\n",
    "X = []\n",
    "for dr in os.listdir(path_save):\n",
    "    for fil in os.listdir(os.path.join(path_save, dr)):\n",
    "        if \"Green\" in fil:\n",
    "            y.append(np.array([0,0,1]))\n",
    "        elif \"Red\" in fil:\n",
    "            y.append(np.array([1,0,0]))\n",
    "        elif \"Yellow\" in fil:\n",
    "            y.append(np.array([0,1,0]))\n",
    "        else:\n",
    "            continue\n",
    "        image = cv2.imread(os.path.join(path_save, dr, fil))\n",
    "        X.append(np.array(image))\n",
    "        \n",
    "\n",
    "X_train = np.array(X)\n",
    "y_train = np.array(y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 50, 50, 32)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 25, 25, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,188,835\n",
      "Trainable params: 1,188,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4563 samples, validate on 1141 samples\n",
      "Epoch 1/15\n",
      "4563/4563 [==============================] - 13s 3ms/step - loss: 2.4626 - acc: 0.8466 - val_loss: 2.3873 - val_acc: 0.8519\n",
      "Epoch 2/15\n",
      "4563/4563 [==============================] - 13s 3ms/step - loss: 2.4810 - acc: 0.8448 - val_loss: 2.3873 - val_acc: 0.8519\n",
      "Epoch 3/15\n",
      "4563/4563 [==============================] - 13s 3ms/step - loss: 1.8510 - acc: 0.8753 - val_loss: 0.7323 - val_acc: 0.9509\n",
      "Epoch 4/15\n",
      "4563/4563 [==============================] - 14s 3ms/step - loss: 0.6836 - acc: 0.9489 - val_loss: 0.2224 - val_acc: 0.9833\n",
      "Epoch 5/15\n",
      "4563/4563 [==============================] - 13s 3ms/step - loss: 0.2163 - acc: 0.9785 - val_loss: 0.0594 - val_acc: 0.9921\n",
      "Epoch 6/15\n",
      "4563/4563 [==============================] - 13s 3ms/step - loss: 0.1044 - acc: 0.9862 - val_loss: 0.0401 - val_acc: 0.9947\n",
      "Epoch 7/15\n",
      "4563/4563 [==============================] - 15s 3ms/step - loss: 0.0608 - acc: 0.9908 - val_loss: 0.0346 - val_acc: 0.9947\n",
      "Epoch 8/15\n",
      "4563/4563 [==============================] - 13s 3ms/step - loss: 0.0393 - acc: 0.9934 - val_loss: 0.0308 - val_acc: 0.9974\n",
      "Epoch 9/15\n",
      "4563/4563 [==============================] - 12s 3ms/step - loss: 0.0328 - acc: 0.9952 - val_loss: 0.0274 - val_acc: 0.9974\n",
      "Epoch 10/15\n",
      "4563/4563 [==============================] - 12s 3ms/step - loss: 0.0257 - acc: 0.9956 - val_loss: 0.0300 - val_acc: 0.9956\n",
      "Epoch 11/15\n",
      "4563/4563 [==============================] - 11s 2ms/step - loss: 0.0243 - acc: 0.9963 - val_loss: 0.0202 - val_acc: 0.9982\n",
      "Epoch 12/15\n",
      "4563/4563 [==============================] - 11s 2ms/step - loss: 0.0194 - acc: 0.9980 - val_loss: 0.0225 - val_acc: 0.9974\n",
      "Epoch 13/15\n",
      "4563/4563 [==============================] - 11s 2ms/step - loss: 0.0173 - acc: 0.9985 - val_loss: 0.0216 - val_acc: 0.9982\n",
      "Epoch 14/15\n",
      "4563/4563 [==============================] - 14s 3ms/step - loss: 0.0169 - acc: 0.9982 - val_loss: 0.0202 - val_acc: 0.9982\n",
      "Epoch 15/15\n",
      "4563/4563 [==============================] - 15s 3ms/step - loss: 0.0190 - acc: 0.9978 - val_loss: 0.0222 - val_acc: 0.9982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f24b4df4850>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "input_shape = (50, 50, 3)\n",
    "num_classes = 3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, kernel_size=(2, 2),padding='same', activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, (2, 2), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-28dc01530dbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ruggero/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not JSON Serializable:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2668\u001b[0;31m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_json_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ruggero/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m_updated_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2633\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2635\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2636\u001b[0m         model_config = {\n\u001b[1;32m   2637\u001b[0m             \u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ruggero/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegacy_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMerge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "#loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "#score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "#print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(X_train[0:1,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4563, 50, 50, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image  = cv2.imread(\"00003.png\").reshape(1,50,50,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50, 50, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.70353886e-02,   1.67888993e-31,   9.52964604e-01]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa130737d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
